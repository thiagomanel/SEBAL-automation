#!/bin/bash

# Checking params
if [[ $# -ne 3 ]]; then
  echo "Usage:" $0 "number_of_samples number_of_images number_of_workers"
  exit 1
fi

# Entry params
n_samples=$1
n_images=$2
n_workers=$3

DIRNAME=`dirname $0`
. "$DIRNAME/sebal-automation.conf"

source "$DIRNAME/infra.sh"

# This function checks parameters consistency
function checkParams {
  if [ $n_samples -le 0 ]
  then
    echo $n_samples "is not a valid number of samples"
    exit 1
  fi

  if [ $n_images -le 0 ]
  then
    echo $n_images "is not a valid number of samples"
    exit 1
  fi

  if [ $n_workers -le 0 ]
  then
    echo $n_workers "is not a valid number of samples"
    exit 1
  fi
}

# This function verifies the state of a given image
function stateVerifier {
  local image_name=$1

  execution_monitor_cmd="sudo su postgres -c \"echo -e \"$sebal_db_password\n\" | psql -d $sebal_db_name -U $sebal_db_user -c \"SELECT state FROM nasa_images WHERE image_name = '$image_name';\"\""
  state=$(ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p $scheduler_port -i $private_key_path  $scheduler_user_name@$scheduler_ip ${execution_monitor_cmd})
  sed -i '1d' $state
  sed -i '1d' $state
  sed -i '$d' $state
  return $state
}

# This function monitors if images have reached a "fetched" state
function monitorExecution {
  images_count=0

  while true
  do
    for i in $n_images
    do
      for j in $n_samples
      do
        state=`stateVerifier $original_image_name"_"$i"_sample_$j"`
        if [ "$state" == "fetched" ]
        then
          echo "Image $original_image_name"_"$i"_sample_$j" was fetched!"
          images_count=$(($images_count+1))
        fi

        if [ $images_count -ge $(($n_images * $n_samples)) ]
        then
          echo "All images were processed!"
          return 0
        fi
      done
    done
  done
}

#TODO we need to abort execution if a command fails

checkParams
checkComponentsResponse
runScheduler
runCrawler
runFetcher
checkIfComponentsAreRunning
monitorExecution
gatherOutputData

# data/input diretorio com as imagens de entrada

#TODO assumindo que a infra eh levantado por outro meio
#TODO no inicio do experimento:
#	copiar data/input para um diretorio do crawler (feito)

#TODO antes de cada sample:
#	ter certeza que sebal-engine estah idle
#	ter certeza que o crawler, o fetcher e o scheduler estao up and running (50%)
#	criar os fake dirs baseado com crawler:/path/data/input (eh preciso dar um nome para esse diretorio) (?)
#	pre-allocar o numero de workers corretos no pool (feito)
#	criar as entradas correspondentes no BD, no estado correto. isso deve engatilhar a execucao (feito)
#	monitorar ate que todas estejam no estado final (fetched?). (feito)
#	gather os dados dos experimento para a maquina local

#TODO: output precisa de um padrao. n_sample + random (stamp?)
